# convolution

## 算子operator
### 多通道卷积
> 一个卷积核，包含(输入通道)个通道，对应于输入通道的卷积核每个通道的参数不同。一个卷积核(包含输入通道个通道)只输出一个通道，(对应于输入通道的通道 的计算结果相加)
>
>输入：[N,C_in,H_in,W_in]
>
>卷积块：[C_out,C_in,H_k,W_k] #作用于每个样本，n in N
>
>输出：[N,C_out,H_out,H_in]
>
>权重：[C_out,C_in,H_out,H_in]
>
>偏执：[C_out]

### 池化
> 使用某一位置的相邻输出的总体统计特征代替网络在该位置的输出，其好处是当输入数据做出少量平移时，经过池化函数后的大多数输出还能保持不变

## network
### ZFNet
问题
> AlexNet第一层中有大量的高频（边缘）和低频（非边缘）信息的混合，却几乎没有覆盖到中间的频率信息
>
> 由于第一层卷积用的步长为4，太大，导致了有非常多的混叠情况，学到的特征不是特别好看，不像是后面的特征能看到一些纹理、颜色等

改进

> 将AlexNet的第一层的卷积核大小从11*11改成7*7
>
> 将第一个卷积层的卷积核滑动步长4从改成2
>
> 改进网络结构

### VGG
> 训练多种网络，采用预训练的方式，，即先训练一部分小网络，然后确保这部分网络收敛之后再在这个基础上逐渐加深
>
> 所有隐藏层都使用了ReLU激活函数，而不是LRN(Local Response Normalization)(AlexNet中出现的)，因为LRN浪费了更多了内存和时间并且性能没有太大提升
>
>使用更小的卷积核和更小的滑动步长。和AlexNet相比，VGG的卷积核大小只有3*3和1*1两种。卷积核的感受野很小，因此可以把网络加深，同时使用多个小卷积核使得网络总参数量也减少了
>
>3*3卷积核相比于一个大尺寸的卷积核有更多的非线性函数，使得模型更有判别性
>
>同时，多个3*3层比一个大的卷积核参数更少;
>
>为什么3个3 * 3卷积核可以代替一个7 * 7卷积核，这是因为这两者的感受野是一致的，并且多个3*3小卷积核非线性更多，效果更好
>
>1*1卷积的引入是在不影响输入输出维数的情况下，对输入进行线性形变，然后通过Relu进行非线性处理，增加网络的非线性表达能力
