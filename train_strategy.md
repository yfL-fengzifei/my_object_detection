# train_strategy

##梯度消失
> 经过反向传播之后，梯度值衰减到接近于零的现象作为梯度消失；(就是当应用激活函数的时候，反向传播时，当神经网络的参数随机初始化的时，x很有可能取值在数值很大或很小的区域，这些地方都会造成sigmoid函数的导数接近0,导致x的梯度接近于0，或者后面层的梯度将衰减到非常小的位置)，relu不会

##Batch Normalization
> 2015年
>
> 目的是对神经网络中间层的输出进行标准化处理，使得中间层的输出更加稳定
>
> 一般，对神经网络的数据进行标准化处理，处理后的样本数据集满足均值为0，方差为1的统计分布，这是因为当输入数据的分布比较固定时，有利于算法的稳定和收敛
>
> 对于深度神经网络来说，由于参数是不断更新的，即使输入数据已经做过标准化处理，但是对于比较靠后的那些层，其接受的输入仍然是剧烈变化的，通常导致数值不稳定，模型很难收敛
>
> BatchNorm能够使神经网络中间层的输出更加稳定。主要思路就是，在训练时按mini-batch为单位，对神经元的数值进行归一化，是数据满足均值为0，方差为1.优点：
>> 使学习快速进行，(能够使用较大的学习率)
>>
>> 降低模型对初始值的敏感性
>> 
>> 从一定程度上抑制过拟合
>
> 具体步骤：
>
> 1.计算mini-batch内样本的均值
>> 注意，每个样本有多个特征，则对每个特征分别计算mini-batch内样本的均值，返回的维度也是特征数
>
> 2.计算mini-batch内样本的方差
>> 对每个特征分别计算mini-batch内样本的方差，返回的维度也是特征数
>
> 3.计算标准化的输出
>> 利用均值和方差，对输入数据做归一化
>>
>>注意归一化时分母有一个微小量为了防止分母为0
>>
>> 对每个样本，利用每个特征的均值和方差，来计算标准化后的输出
>
> 注意：
>> 如果强行限制输出层的分布是标准化的，可能会导致某些特征模式的丢失，所以在标准化之后，BN会紧接着对数据做缩放和平移
>>
>> y_i={\gamma} * x_i + {\beta} 
>> 
>> 此时x_i是经过标准化的样本数据，{\gamma}和{\beta}是可学习的参数，可以赋初值为{\gamma}=1，{\beta}=0,然后在训练的过程中不断学习调整
>
> 例1
>> 全连接层：
>> 输入 x [N,K]
>>
>> 输出 y [N,K]
>> 
>> 均值 [K,]
>> 
>> 方差 [K,]
>> 
>> 缩放系数{\gamma} [K,]
>> 
>> 平移系数{\beta} [K,]
>
> 例2
>> 卷积层：沿着通道C维度进行展开，即分为对每个通道计算N个样本中总共N*H*W个像素点的均值和方差(就是硬算均值和方差)
>> 输入 x [N,C,H,W]
>>
>> 输出 y [N,C,H,W]
>> 
>> 均值 [C,]
>> 
>> 方差 [C,]
>> 
>> 缩放系数{\gamma} [C,]
>> 
>> 平移系数{\beta} [C,]
>>
>
> 注意：
>> 如果使用同样的方法对需要预测的一批样本进行归一化，则预测结果会出现不确定性，这是因为样本的组织，即batch的组织是不同的。
>>
>> 解决的办法就是在训练过程中将大量样本的均值和方差保存下来，预测时直接使用保存好的值而不再重新计算，实际上，BN的具体实现中，训练时会计算均值和方差的移动平均值，
>>
>> **paddle的具体实现见文档**

## dropout
> 抑制过拟合的方法，其做法是在神经网络学习过程中，随机删除一部分神经元，将其输出设置为0.何止一种正则化手段，用过在训练过程中阻止神经元节点间的相关性来减少过拟合
>
> 引入另一个问题：训练时，由于部分神经元被随机丢弃，输出数据的总大小会变小，但训练时没有丢弃神经元，导致训练和预测时数据的分布不一样
> 
> 解决
>> 1. downgrade_in_infer:
>>> 训练时以比例r随机丢弃一部分神经元，不向后传递所有神经元的信号，预测时向后传递所有神经元的信号，但是将每个神经元上的数据**乘以**(1-r)
>>
>> 2. unsacle_in_train
>>> 训练时以比例r丢弃一些神经元，不向后传递信号，但是将那些被保留的神经元上的数值**除以**(1-r) 
